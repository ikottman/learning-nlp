{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the first leg of my Natural Language Processing (NLP) journey. I will be covering practical use cases for NLP along with common techniques to achieve them. My examples will use Python, along with [Jupyter](http://jupyter.org/) and [spaCy](https://spacy.io/).\n",
    "\n",
    "First, I'll start off with some common components of a language processing pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization is the process of breaking up text into smaller pieces, called _tokens_. Tokens may or may not be words. There is not one specific way to perform tokenization, as different problems may call for different granularity of tokens. Below are two examples of tokenizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I'm against picketing, but I don't know how to show it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"I'm\", 'against', 'picketing,', 'but', 'I', \"don't\", 'know', 'how', 'to', 'show', 'it.']\n"
     ]
    }
   ],
   "source": [
    "# A naÃ¯ve tokenizer that splits on spaces.\n",
    "tokens = text.split(\" \")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just splitting on spaces gets you 80% of the way there, but it doesn't take into account things like punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', \"'m\", 'against', 'picketing', ',', 'but', 'I', 'do', \"n't\", 'know', 'how', 'to', 'show', 'it', '.']\n"
     ]
    }
   ],
   "source": [
    "# The default tokenizer in spaCy is a bit more robust.\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "\n",
    "english = spacy.load('en')\n",
    "tokenizer = English().Defaults.create_tokenizer(english)\n",
    "\n",
    "tokens = tokenizer(text)\n",
    "tokens = [token.text for token in tokens]\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It treats words and symbols as separate tokens, and even splits up contractions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop words are merely words you want your NLP pipeline to ignore. Stop lists usually contain common words like 'the', but they may also contain domain-specific words like 'Cerner' or 'DevCon'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stopwords in spaCy: 305\n",
      "Examples: no to meanwhile latter over none seem ten thus within\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "\n",
    "stopwords = list(English.Defaults.stop_words)\n",
    "\n",
    "print(f\"Number of stopwords in spaCy: {len(stopwords)}\")\n",
    "print(f\"Examples: {' '.join(stopwords[0:10])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I, 'm, picketing, ,, I, n't, know, .]\n"
     ]
    }
   ],
   "source": [
    "text = \"I'm against picketing, but I don't know how to show it.\"\n",
    "\n",
    "english = spacy.load('en')\n",
    "doc = english(text)\n",
    "\n",
    "# spaCy automatically tags when words are stop words with `is_stop`\n",
    "tokens = [token for token in doc if not token.is_stop]\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two common ways of combining similar words are _stemming_ and _lemmatization_.\n",
    "\n",
    "## Stemming\n",
    "\n",
    "The simpler of the two, stemming works by chopping off the end of each word. Porter's algorithm is a popular implementation of stemming. It would convert `ponies` -> `poni` and `cats` -> `cat`. Stemming is fast but can have a high false positive rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thi\n",
      "shirt\n",
      "is\n",
      "dry-clean\n",
      "onli\n",
      "which\n",
      "mean\n",
      "it'\n",
      "dirti\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "text = \"This shirt is dry-clean only which means it's dirty\"\n",
    "tokens = text.split(\" \")\n",
    "singles = [stemmer.stem(token) for token in tokens]\n",
    "for token in tokens:\n",
    "    stem = stemmer.stem(token)\n",
    "    print(stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming doesn't take into account parts of speech, and therefore can incorrectly group different words together. [This is why](https://github.com/explosion/spaCy/issues/327#issuecomment-208658745) spaCy doesn't include a stemmer, instead it provides lemmatization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "Grouping words into their _lemma_, or standard form of the word. For example, `better` and `best` have the lemma `good` whereas `walk` is the lemma for `walking`, `walked`, and `walks`. Lemmatization is more complicated than stemming, but has fewer false positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I -> -PRON-\n",
      "n't -> not\n",
      "slept -> sleep\n",
      "days -> day\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "\n",
    "text = \"I haven't slept for ten days, because that would be too long.\"\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "doc = nlp(text)\n",
    "for token in doc:\n",
    "    if not token.is_punct and token.text != token.lemma_:\n",
    "        print(token.text, \"->\", token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see plurals and different tenses are collapsed into a single base form. spaCy also introduces a special case for pronouns because there is no clear lemma for pronouns. Should \"I\" become \"me\", \"it\", or \"they\"? To avoid this ambiguity they treat all pronouns as a special lemma, -PRON-."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
