{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization is the process of breaking up text into smaller pieces, called _tokens_. Tokens may or may not be words. There is not one specific way to perform tokenization, as different problems may call for different granluarity of tokens. Below are two examples of tokenizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I'm against picketing, but I don't know how to show it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"I'm\", 'against', 'picketing,', 'but', 'I', \"don't\", 'know', 'how', 'to', 'show', 'it.']\n"
     ]
    }
   ],
   "source": [
    "# A na√Øve tokenizer that splits on spaces.\n",
    "tokens = text.split(\" \")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just splitting on spaces gets you 80% of the way there, but it doesn't take into account things like punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', \"'m\", 'against', 'picketing', ',', 'but', 'I', 'do', \"n't\", 'know', 'how', 'to', 'show', 'it', '.']\n"
     ]
    }
   ],
   "source": [
    "# The default tokenizer in spaCy is a bit more robust.\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "\n",
    "english = spacy.load('en')\n",
    "tokenizer = English().Defaults.create_tokenizer(english)\n",
    "\n",
    "tokens = tokenizer(text)\n",
    "tokens = [token.text for token in tokens]\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It treats words and symbols as separate tokens, and even splits up contractions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
